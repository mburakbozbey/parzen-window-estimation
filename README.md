# Parzen Window Estimation using Gaussian Kernels for Multiclass Classification
## Procedure:

The multivariate kernel density estimator is the estimated pdf of a random sample vector. Let ğ‘¥ be a ğ‘‘-dimensional random vector with a
density function ğ‘“ and let ğ‘¦ğ‘– be a random sample drawn from ğ‘“ for ğ‘– = 1, 2, â€¦ , ğ‘›, where n is the number of random samples. For any real
vectors of ğ‘¥, the kernel density estimation is:

<p align="center">
  <img src="https://i.ibb.co/x2YqChf/1.png">
</p>

where the kernel functions is:

<p align="center">
  <img src="https://i.ibb.co/sb1CbWp/2.png">
</p>

and H is the d-by-d variance matrix. In MATLAB, I used mvksdensity function which uses a diagonal variance matrix and a product kernel. That is, H1/2 is a square diagonal matrix with the elements of vector (h1, h2, â€¦, hd) on the main diagonal. K(x) takes the product form K(x) = k(x1)k(x2)â‹¯k(xd), where k(Â·) is a one-dimensional Gaussian kernel function. Then, the multivariate kernel density estimator becomes,

<p align="center">
  <img src="https://i.ibb.co/R7MdwbJ/3.png">
</p>

In this part, I used standard multivariate gaussian kernel where H represents the covariance matrix :

<p align="center">
  <img src="https://i.ibb.co/hccFn8h/4.png">
</p>

For each class, we can compare resulting pdfsâ€™ multiplied by prior probabilities in natural logarithm. While ğ‘”ğ‘– (ğ‘¥âƒ—),

                                                    ğ‘”ğ‘– (ğ‘¥âƒ—) = ğ‘™ğ‘› ğ‘ƒ(ğ‘¥âƒ— | ğ‘¤ğ‘– ) + ğ‘™ğ‘› ğ‘ƒ(ğ‘¤ğ‘– )
                                                    
After iterating over all classesâ€™ resulting if, ğ‘”ğ‘– (ğ‘¥âƒ—) > ğ‘”ğ‘— (ğ‘¥âƒ—), then we assign x to the class ğ‘¤ğ‘–. On a **private dataset**, recall and precision for test set:

<p align="center">
  <img src="https://i.ibb.co/tsn16VG/6.png">
</p>

## Note: 
For overall comparison of algorithms on same **private dataset** please visit [Hierarchical Clustering with SVM](https://github.com/mburakbozbey/hierarchical-clustering-with-svm) repository.
